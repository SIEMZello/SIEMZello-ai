{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75797397",
   "metadata": {},
   "source": [
    "## Testing the ClassificationModel Class\n",
    "\n",
    "In this notebook, we will test our `ClassificationModel` class to ensure it correctly classifies attack types in network traffic. The steps involved in this process are as follows:\n",
    "\n",
    "1. **Set up the environment**: Import necessary libraries and add the project root to the Python path\n",
    "2. **Load sample data**: Use the sample data we created in the previous tests\n",
    "3. **Initialize the model**: Create an instance of the `ClassificationModel` class\n",
    "4. **Generate predictions**: Use the model to classify attack types in the sample network traffic\n",
    "5. **Analyze the results**: Verify the model provides sensible attack classification results\n",
    "\n",
    "Let's start by setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee0ead9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\HAMZA\\Desktop\\smartshield\\MLEngine-main\\MLEngine-main\\network_traffic_anomaly_detection\n",
      "Successfully imported ClassificationModel class\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Import our classification model\n",
    "from src.models.classification_model import ClassificationModel\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(\"Successfully imported ClassificationModel class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4cd413",
   "metadata": {},
   "source": [
    "## Load Sample Data\n",
    "\n",
    "We'll load the sample data we created and saved in the previous tests. If the file doesn't exist, we'll generate new sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f22f4ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 sample records from data/sample_traffic_data.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dur</th>\n",
       "      <th>proto</th>\n",
       "      <th>service</th>\n",
       "      <th>state</th>\n",
       "      <th>spkts</th>\n",
       "      <th>dpkts</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>rate</th>\n",
       "      <th>sttl</th>\n",
       "      <th>...</th>\n",
       "      <th>sload</th>\n",
       "      <th>dload</th>\n",
       "      <th>sloss</th>\n",
       "      <th>dloss</th>\n",
       "      <th>sinpkt</th>\n",
       "      <th>dinpkt</th>\n",
       "      <th>sjit</th>\n",
       "      <th>djit</th>\n",
       "      <th>smean</th>\n",
       "      <th>dmean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.938536</td>\n",
       "      <td>ospf</td>\n",
       "      <td>smtp</td>\n",
       "      <td>CON</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>4043</td>\n",
       "      <td>4397</td>\n",
       "      <td>82</td>\n",
       "      <td>94</td>\n",
       "      <td>...</td>\n",
       "      <td>1.897377</td>\n",
       "      <td>3.565321</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.024646</td>\n",
       "      <td>0.043474</td>\n",
       "      <td>0.008167</td>\n",
       "      <td>0.037557</td>\n",
       "      <td>538</td>\n",
       "      <td>709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.020243</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>REQ</td>\n",
       "      <td>89</td>\n",
       "      <td>71</td>\n",
       "      <td>7655</td>\n",
       "      <td>1095</td>\n",
       "      <td>53</td>\n",
       "      <td>118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.596839</td>\n",
       "      <td>1.889905</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.053873</td>\n",
       "      <td>0.036353</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.005294</td>\n",
       "      <td>302</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.633491</td>\n",
       "      <td>arp</td>\n",
       "      <td>-</td>\n",
       "      <td>REQ</td>\n",
       "      <td>60</td>\n",
       "      <td>44</td>\n",
       "      <td>3173</td>\n",
       "      <td>7729</td>\n",
       "      <td>24</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100274</td>\n",
       "      <td>1.279162</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.214798</td>\n",
       "      <td>0.017991</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>283</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.825885</td>\n",
       "      <td>udp</td>\n",
       "      <td>http</td>\n",
       "      <td>INT</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>1121</td>\n",
       "      <td>9567</td>\n",
       "      <td>26</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>0.463335</td>\n",
       "      <td>0.269168</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.039207</td>\n",
       "      <td>0.076376</td>\n",
       "      <td>0.002929</td>\n",
       "      <td>0.004232</td>\n",
       "      <td>222</td>\n",
       "      <td>851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.339250</td>\n",
       "      <td>arp</td>\n",
       "      <td>http</td>\n",
       "      <td>CON</td>\n",
       "      <td>9</td>\n",
       "      <td>47</td>\n",
       "      <td>3943</td>\n",
       "      <td>1116</td>\n",
       "      <td>89</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>1.105157</td>\n",
       "      <td>0.295806</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013021</td>\n",
       "      <td>0.066326</td>\n",
       "      <td>0.002835</td>\n",
       "      <td>0.010061</td>\n",
       "      <td>500</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        dur proto service state  spkts  dpkts  sbytes  dbytes  rate  sttl  \\\n",
       "0  0.938536  ospf    smtp   CON      4     60    4043    4397    82    94   \n",
       "1  6.020243   udp       -   REQ     89     71    7655    1095    53   118   \n",
       "2  2.633491   arp       -   REQ     60     44    3173    7729    24   100   \n",
       "3  1.825885   udp    http   INT     14      8    1121    9567    26    38   \n",
       "4  0.339250   arp    http   CON      9     47    3943    1116    89   117   \n",
       "\n",
       "   ...     sload     dload  sloss  dloss    sinpkt    dinpkt      sjit  \\\n",
       "0  ...  1.897377  3.565321      0      2  0.024646  0.043474  0.008167   \n",
       "1  ...  0.596839  1.889905      3      2  0.053873  0.036353  0.005172   \n",
       "2  ...  0.100274  1.279162      4      3  0.214798  0.017991  0.000671   \n",
       "3  ...  0.463335  0.269168      3      1  0.039207  0.076376  0.002929   \n",
       "4  ...  1.105157  0.295806      4      1  0.013021  0.066326  0.002835   \n",
       "\n",
       "       djit  smean  dmean  \n",
       "0  0.037557    538    709  \n",
       "1  0.005294    302    297  \n",
       "2  0.000336    283    610  \n",
       "3  0.004232    222    851  \n",
       "4  0.010061    500    243  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to sample data\n",
    "sample_path = \"data/sample_traffic_data.parquet\"\n",
    "\n",
    "# Check if the sample data file exists\n",
    "if os.path.exists(sample_path):\n",
    "    # Load the sample data\n",
    "    sample_data = pd.read_parquet(sample_path)\n",
    "    print(f\"Loaded {len(sample_data)} sample records from {sample_path}\")\n",
    "else:\n",
    "    print(f\"Sample data file {sample_path} not found. Generating new sample data...\")\n",
    "    \n",
    "    # Define function to generate sample data\n",
    "    def generate_sample_data(n_samples=10, random_state=42):\n",
    "        \"\"\"Generate synthetic network traffic data for testing\"\"\"\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        synthetic_data = {\n",
    "            'dur': np.random.exponential(2, n_samples),\n",
    "            'proto': np.random.choice(['tcp', 'udp', 'icmp', 'arp', 'ospf'], n_samples),\n",
    "            'service': np.random.choice(['-', 'dns', 'http', 'smtp', 'ftp', 'ssh'], n_samples),\n",
    "            'state': np.random.choice(['INT', 'FIN', 'CON', 'REQ', 'RST'], n_samples),\n",
    "            'spkts': np.random.randint(1, 100, n_samples),\n",
    "            'dpkts': np.random.randint(1, 100, n_samples),\n",
    "            'sbytes': np.random.randint(100, 10000, n_samples),\n",
    "            'dbytes': np.random.randint(100, 10000, n_samples),\n",
    "            'rate': np.random.randint(1, 100, n_samples),\n",
    "            'sttl': np.random.randint(30, 255, n_samples),\n",
    "            'dttl': np.random.randint(30, 255, n_samples),\n",
    "            'sload': np.random.exponential(1, n_samples),\n",
    "            'dload': np.random.exponential(1, n_samples),\n",
    "            'sloss': np.random.randint(0, 5, n_samples),\n",
    "            'dloss': np.random.randint(0, 5, n_samples),\n",
    "            'sinpkt': np.random.exponential(0.1, n_samples),\n",
    "            'dinpkt': np.random.exponential(0.1, n_samples),\n",
    "            'sjit': np.random.exponential(0.01, n_samples),\n",
    "            'djit': np.random.exponential(0.01, n_samples),\n",
    "            'smean': np.random.randint(100, 1000, n_samples),\n",
    "            'dmean': np.random.randint(100, 1000, n_samples),\n",
    "        }\n",
    "        \n",
    "        # Create DataFrame\n",
    "        return pd.DataFrame(synthetic_data)\n",
    "    \n",
    "    # Generate sample data\n",
    "    sample_data = generate_sample_data(n_samples=10)\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "    # Save the sample data\n",
    "    sample_data.to_parquet(sample_path, index=False)\n",
    "    print(f\"Generated and saved {len(sample_data)} sample records to {sample_path}\")\n",
    "\n",
    "# Display the first few rows\n",
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc44fb06",
   "metadata": {},
   "source": [
    "## Initialize and Test the ClassificationModel\n",
    "\n",
    "Now we'll create an instance of our `ClassificationModel` class and use it to classify attack types in the sample network traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c9b930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at c:\\Users\\HAMZA\\Desktop\\smartshield\\MLEngine-main\\MLEngine-main\\network_traffic_anomaly_detection\\models\\classification_model.cbm\n",
      "Using classification model: c:\\Users\\HAMZA\\Desktop\\smartshield\\MLEngine-main\\MLEngine-main\\network_traffic_anomaly_detection\\models\\classification_model.cbm\n",
      "Model loaded with 24 features\n",
      "Successfully initialized ClassificationModel\n",
      "Using model file: c:\\Users\\HAMZA\\Desktop\\smartshield\\MLEngine-main\\MLEngine-main\\network_traffic_anomaly_detection\\models\\classification_model.cbm\n"
     ]
    }
   ],
   "source": [
    "# Check if we need to run setup_models.py to create dummy models\n",
    "model_path = os.path.join(project_root, \"models\", \"classification_model.cbm\")\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Model file not found: {model_path}\")\n",
    "    print(\"Running setup_models.py to create dummy models...\")\n",
    "    \n",
    "    # Change directory to project root\n",
    "    os.chdir(project_root)\n",
    "    \n",
    "    # Run setup_models.py to create dummy models\n",
    "    from setup_models import setup_models\n",
    "    setup_models()\n",
    "    \n",
    "    print(\"Dummy models created successfully\")\n",
    "else:\n",
    "    print(f\"Found model file at {model_path}\")\n",
    "\n",
    "# Initialize the classification model\n",
    "classification_model = ClassificationModel()\n",
    "\n",
    "print(\"Successfully initialized ClassificationModel\")\n",
    "print(f\"Using model file: {classification_model.model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f56b801",
   "metadata": {},
   "source": [
    "## Generate Attack Type Classifications\n",
    "\n",
    "Let's use the model to classify the specific attack types for our sample traffic.\n",
    "\n",
    "Note: Since we created this synthetic data randomly, it doesn't contain actual attack signatures. The model will classify it based on the patterns it learned during training, but we should only consider these results as a technical validation of the model's functionality rather than accurate classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c925bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (10, 21) with columns: ['dur', 'proto', 'service', 'state', 'spkts']...\n",
      "Preprocessed data shape: (10, 24)\n",
      "Input data shape: (10, 21) with columns: ['dur', 'proto', 'service', 'state', 'spkts']...\n",
      "Preprocessed data shape: (10, 24)\n",
      "Error generating predictions: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HAMZA\\AppData\\Local\\Temp\\ipykernel_2064\\748960045.py\", line 11, in <module>\n",
      "    results['predicted_attack_type'] = attack_types\n",
      "  File \"c:\\Users\\HAMZA\\.conda\\envs\\cuda_test\\lib\\site-packages\\pandas\\core\\frame.py\", line 4311, in __setitem__\n",
      "    self._set_item(key, value)\n",
      "  File \"c:\\Users\\HAMZA\\.conda\\envs\\cuda_test\\lib\\site-packages\\pandas\\core\\frame.py\", line 4524, in _set_item\n",
      "    value, refs = self._sanitize_column(value)\n",
      "  File \"c:\\Users\\HAMZA\\.conda\\envs\\cuda_test\\lib\\site-packages\\pandas\\core\\frame.py\", line 5267, in _sanitize_column\n",
      "    arr = sanitize_array(value, self.index, copy=True, allow_2d=True)\n",
      "  File \"c:\\Users\\HAMZA\\.conda\\envs\\cuda_test\\lib\\site-packages\\pandas\\core\\construction.py\", line 606, in sanitize_array\n",
      "    subarr = maybe_infer_to_datetimelike(data)\n",
      "  File \"c:\\Users\\HAMZA\\.conda\\envs\\cuda_test\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\", line 1181, in maybe_infer_to_datetimelike\n",
      "    raise ValueError(value.ndim)  # pragma: no cover\n",
      "ValueError: 2\n"
     ]
    }
   ],
   "source": [
    "# Generate attack type predictions\n",
    "try:\n",
    "    # Get attack type predictions\n",
    "    attack_types = classification_model.predict(sample_data)\n",
    "    \n",
    "    # Flatten the predictions if they're in a 2D array\n",
    "    if isinstance(attack_types, np.ndarray) and attack_types.ndim > 1:\n",
    "        attack_types = attack_types.flatten()\n",
    "    \n",
    "    # Get attack type probabilities\n",
    "    attack_type_probs = classification_model.predict_proba(sample_data)\n",
    "    \n",
    "    # Add predictions to the sample data\n",
    "    results = sample_data.copy()\n",
    "    results['predicted_attack_type'] = attack_types\n",
    "    \n",
    "    # Display prediction counts\n",
    "    attack_type_counts = pd.Series(attack_types).value_counts()\n",
    "    print(\"Predicted attack types:\")\n",
    "    for attack_type, count in attack_type_counts.items():\n",
    "        print(f\"  {attack_type}: {count} records\")\n",
    "    \n",
    "    # Display the results\n",
    "    print(\"\\nPrediction results:\")\n",
    "    print(results[['proto', 'service', 'state', 'predicted_attack_type']].head())\n",
    "    \n",
    "    # Check if we have classes and probability values\n",
    "    if hasattr(classification_model.model, 'classes_'):\n",
    "        print(\"\\nModel classes:\")\n",
    "        print(classification_model.model.classes_)\n",
    "        \n",
    "        # Add probabilities for each class to results if possible\n",
    "        if len(attack_type_probs) > 0 and isinstance(attack_type_probs[0], np.ndarray):\n",
    "            prob_cols = []\n",
    "            for i, cls in enumerate(classification_model.model.classes_):\n",
    "                col_name = f\"prob_{cls}\"\n",
    "                results[col_name] = attack_type_probs[:, i]\n",
    "                prob_cols.append(col_name)\n",
    "                \n",
    "            # Show probability columns\n",
    "            print(\"\\nProbability columns added:\")\n",
    "            print(prob_cols)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error generating predictions: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c29be9",
   "metadata": {},
   "source": [
    "## Visualize the Results\n",
    "\n",
    "Let's create a visualization to better understand the model's classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33612e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart of attack type predictions\n",
    "try:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    attack_type_counts.plot.bar(color='skyblue', edgecolor='black')\n",
    "    plt.title('Predicted Attack Types')\n",
    "    plt.xlabel('Attack Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error visualizing attack types: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c601cfe",
   "metadata": {},
   "source": [
    "## Visualize Attack Type Probabilities\n",
    "\n",
    "If we have probability information for each class, let's visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe2770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have probability columns\n",
    "prob_columns = [col for col in results.columns if col.startswith('prob_')]\n",
    "\n",
    "if prob_columns:\n",
    "    # Select the first 5 samples for probability visualization\n",
    "    sample_indices = range(5)\n",
    "    \n",
    "    # Create a heatmap of probabilities for these samples\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Prepare probability data for heatmap\n",
    "    prob_data = results.loc[sample_indices, prob_columns].copy()\n",
    "    prob_data.columns = [col.replace('prob_', '') for col in prob_columns]\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(prob_data, annot=True, cmap='YlGnBu', fmt='.2f', linewidths=.5)\n",
    "    plt.title('Attack Type Probabilities for First 5 Samples')\n",
    "    plt.xlabel('Attack Type')\n",
    "    plt.ylabel('Sample Index')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No probability information available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343b3199",
   "metadata": {},
   "source": [
    "## Test Model Reliability\n",
    "\n",
    "Let's check if the model produces consistent results across multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48ff729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions multiple times to check consistency\n",
    "num_runs = 3\n",
    "all_predictions = []\n",
    "\n",
    "for i in range(num_runs):\n",
    "    predictions = classification_model.predict(sample_data)\n",
    "    all_predictions.append(predictions)\n",
    "\n",
    "# Check if all prediction runs gave the same results\n",
    "predictions_match = all(np.array_equal(all_predictions[0], pred) for pred in all_predictions)\n",
    "\n",
    "if predictions_match:\n",
    "    print(\"✅ Model produces consistent predictions across multiple runs\")\n",
    "else:\n",
    "    print(\"⚠️ Warning: Model produces different predictions across runs\")\n",
    "    \n",
    "    # Show differences if any\n",
    "    for i in range(num_runs-1):\n",
    "        differences = np.sum(all_predictions[i] != all_predictions[i+1])\n",
    "        if differences > 0:\n",
    "            print(f\"Run {i+1} vs Run {i+2}: {differences} differences in predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb176740",
   "metadata": {},
   "source": [
    "## Analyze Feature Importance\n",
    "\n",
    "Let's check if we can get feature importance information from this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb17bb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model has feature_importances_ attribute\n",
    "if hasattr(classification_model.model, 'feature_importances_'):\n",
    "    # Get feature importances\n",
    "    feature_importances = classification_model.model.feature_importances_\n",
    "    feature_names = classification_model.model.feature_names_\n",
    "    \n",
    "    # Create a DataFrame for better visualization\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Display the top 10 most important features\n",
    "    print(\"Top 10 most important features:\")\n",
    "    print(importance_df.head(10))\n",
    "    \n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))\n",
    "    plt.title('Top 20 Most Important Features for Attack Classification')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Model doesn't provide feature importances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06885c87",
   "metadata": {},
   "source": [
    "## Integrated Model Testing\n",
    "\n",
    "In a real-world scenario, we would typically use the DetectionModel first to identify potential attacks, and then use the ClassificationModel only on those records flagged as attacks. Let's simulate this two-stage approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e989d899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the detection model\n",
    "from src.models.detection_model import DetectionModel\n",
    "\n",
    "try:\n",
    "    # Initialize the detection model\n",
    "    detection_model = DetectionModel()\n",
    "\n",
    "    # Two-stage approach\n",
    "    # Stage 1: Detect if traffic is an attack\n",
    "    is_attack = detection_model.predict(sample_data)\n",
    "    \n",
    "    # Flatten is_attack if it's 2D\n",
    "    if isinstance(is_attack, np.ndarray) and is_attack.ndim > 1:\n",
    "        is_attack = is_attack.flatten()\n",
    "        \n",
    "    attack_probs = detection_model.predict_proba(sample_data)\n",
    "    \n",
    "    # Create a DataFrame with the results\n",
    "    two_stage_results = sample_data.copy()\n",
    "    two_stage_results['is_attack'] = is_attack\n",
    "    two_stage_results['attack_probability'] = attack_probs\n",
    "    \n",
    "    # Stage 2: Only classify the type for detected attacks\n",
    "    attack_indices = np.where(is_attack == 1)[0]\n",
    "    \n",
    "    if len(attack_indices) > 0:\n",
    "        # Select only the attack traffic\n",
    "        attack_traffic = sample_data.iloc[attack_indices]\n",
    "        \n",
    "        # Classify the attack types\n",
    "        attack_types = classification_model.predict(attack_traffic)\n",
    "        \n",
    "        # Flatten attack_types if it's 2D\n",
    "        if isinstance(attack_types, np.ndarray) and attack_types.ndim > 1:\n",
    "            attack_types = attack_types.flatten()\n",
    "        \n",
    "        # Add attack types to the results\n",
    "        for i, idx in enumerate(attack_indices):\n",
    "            two_stage_results.loc[idx, 'attack_type'] = attack_types[i]\n",
    "            \n",
    "        print(f\"Two-stage approach detected {len(attack_indices)} attacks out of {len(sample_data)} traffic records\")\n",
    "        print(\"\\nResults of two-stage approach:\")\n",
    "        print(two_stage_results[['proto', 'service', 'is_attack', 'attack_probability', 'attack_type']].head(10))\n",
    "    else:\n",
    "        print(\"No attacks detected in the sample data\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in two-stage approach: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c26d19",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have successfully tested our `ClassificationModel` class and verified that:\n",
    "\n",
    "1. The model initializes correctly and loads the required resources\n",
    "2. It can classify network traffic attacks into specific attack types\n",
    "3. We've demonstrated how it integrates with the `DetectionModel` in a two-stage approach\n",
    "\n",
    "This confirms that our attack classification model is working as expected and is ready for use in a production environment as part of a comprehensive network traffic anomaly detection system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
